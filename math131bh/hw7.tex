\documentclass[12pt]{article}

\input{../kz}

\rhead{Math 131BH}

\makeatletter
\def\@seccntformat#1{%
  \expandafter\ifx\csname c@#1\endcsname\c@section\else
  \csname the#1\endcsname\quad
  \fi}
\makeatother

\DeclareMathOperator{\Fr}{Fr}
\newcommand{\lra}{\xLeftrightarrow}
\newcommand{\ra}{\xRightarrow}

\begin{document}

\section{Problem 1}

\subsection{Uniformly Converges Fine}

I propose that $f_n \ra{d_\infty} f$ where $f(x)=0$.

To prove this, fix an $\epsilon$.

Since $f_n$ is odd, only consider positive $x$ ($f_n(0)=0$, so we're fine there).

For our value of $n$ to work, we need
\begin{align*}
         & \left|\frac{x}{1+nx^2}\right| < \epsilon \\
  \iff{} & \frac{|x|}{|1+nx^2|} < \epsilon          \\
  \iff{} & x < \epsilon + \epsilon nx^2             \\
  \iff{} & \frac{x-\epsilon}{\epsilon x^2} < n
\end{align*}

If $x \le \epsilon$ the LHS is nonpositive, so now restrict our $x$s to those greater than $\epsilon$:
\[\frac{x-\epsilon}{\epsilon x^2} < \frac{x}{\epsilon x^2} < \frac{1}{x \epsilon} < \frac{1}{\epsilon^2}\]
so choosing $n$ greater than that should make $|f_n(x)| < 0$. $\square$

\pagebreak

\subsection{Derivatives Not So Much}

$f$ is constant, so its derivative is $0$ everywhere.

For the sequence itself,
\[f_n'(x) = \frac{1-nx^2}{\left(1+nx^2\right)^2}\]
and for nonzero $x$
\begin{align*}
  |f_n'(x)|
   & \le \left|\frac{1}{(1+nx^2)^2}\right| + \left|\frac{nx^2}{\left(1+nx^2\right)^2}\right|   \\
   & \le \left|\frac{1}{(1+nx^2)^2}\right| + \left|\frac{1+nx^2}{\left(1+nx^2\right)^2}\right| \\
   & \le \left|\frac{1}{(1+nx^2)^2}\right| + \left|\frac{1}{1+nx^2}\right|
\end{align*}
As long as $x$ is nonzero, this bound and limit tends to $0$ as $n \to \infty$,
but if $x=0$ it stays a constant $1$ no matter how big $n$ is.

\section{Problem 2}

As always, fix an $\epsilon$ (jeez how many have i fixed at this point).

$f_n$ is equicontinuous so $\exists \delta: d_K(x, y) < \delta \implies |f_n(x)-f_n(y)| < \frac{\epsilon}{2}\ \forall n \in \N$.

$K$ is compact, so there's a finite subcover among $\{B_\delta(x) \mid x \in K\}$.
Let the centers of the finite amount of balls chosen be $x_1$ through $x_n$.

$f_n$ pointwise converges to some $f$,
so for each of these $x_i$ $\exists N_i: |f_n(x_i)-f(x)| < \frac{\epsilon}{2}\ \forall n \ge N_i$.
Take the largest $N_i$ among these and make that our overall $N$.

This $N$ should work, since for all $x \in K$ $\exists x_i: d_K(x_i, x) < \delta$ and
\begin{align*}
  |f_n(x)-f(x)|
   & \le |f_n(x)-f_n(x_i)|+|f_n(x_i)-f(x)|       \\
   & \le \frac{\epsilon}{2} + \frac{\epsilon}{2} \\
   & = \epsilon\quad\square
\end{align*}

\pagebreak

\section{Problem 3}

$f_n$ is uniformly bounded, so $\exists M: f_n(x) < M\ \forall n, x$ and
\[|F_n(x)|=\left|\int_{a}^{x} f_n(t)\,dt\right| \le \int_{a}^{b} |f_n(t)|\,dt \le M(b-a)\]
so the new sequence is also uniformly bounded.

Now let's prove $F_n$ is also equicontinuous- fix an $\epsilon$.

If we let $\delta$ be $\frac{\epsilon}{M}$,
for all $x < y$ in $[a, b]$ s.t. $y-x < \delta$ and $n$ we've
\begin{align*}
  |F_n(x)-F_n(y)|
   & = \left|\int_{x}^{y} f_n(t)\,dt\right| \\
   & \le \int_{x}^{y} |f_n(t)|\,dt          \\
   & \le M(y-x)                             \\
   & \le \frac{M\epsilon}{M}                \\
   & = \epsilon
\end{align*}
with these two things proven, by Arzela-Ascoli there does exist a uniformly convergent subsequence. $\square$

\pagebreak

\section{Problem 4}

We were asked to solve this problem, not Exercise 12 in Chap. 6 so I'm just gonna use the results from that without proof.

It STP $\forall \epsilon > 0\ \exists P$ s.t. the integral is less than $\epsilon$.

By that exercise, $\exists g \in C([a, b]): \norm{f-g}_2 < \epsilon^2$, so
\[\int_{a}^{b} |f(x)-g(x)|^2\,dx < \epsilon\]
Since polynomials are dense in $C([a, b])$ $\exists P: |g(x)-P(x)| < \epsilon$, so
\begin{align*}
        & \int_{a}^{b} |f(x)-P(x)|^2\,dx \\
  \le{} & \int_{a}^{b} (|f(x)-g(x)| + |g(x)-P(x)|)^2\,dx                                                   \\
  =     & \int_{a}^{b} |f(x)-g(x)|^2\,dx + \int_{a}^{b} |f(x)-g(x)||g(x)-P(x)|\,dx + \int_{a}^{b} |g(x)-P(x)|^2\,dx \\
  \le{} & \epsilon + \epsilon^2(b-a) + \int_{a}^{b} |f(x)-g(x)||g(x)-P(x)|\,dx \\
  \le{} & \epsilon + \epsilon^2(b-a) + \epsilon\int_{a}^{b} |f(x)-g(x)|\,dx
\end{align*}
The first two terms obviously to go $0$.
As for the third one, since $0 \le |a| \le |a|^2+1$
\begin{align*}
  \epsilon\int_{a}^{b} |f(x)-g(x)|
  &\le \epsilon\int_{a}^{b} |f(x)-g(x)|^2+1\,dx \\
  &\le \epsilon(\epsilon + (b-a))
\end{align*}
so our overall error still goes to $0$. $\square$

\pagebreak

\section{Problem 5}

\subsection{In \texorpdfstring{$C^1$}{C1}}

\subsubsection{Actually Differentiable}

I'm kinda, somewhat, probably sure the derivative is equal to
\[d_n(x)=\int_{-2}^{2} f_n'(x-y)g(y)\,dy\]
but let's start straight from the definition.
It suffices to find for any $\epsilon$ a $\delta$ s.t.
\[\left|\frac{h_n(x+\delta)-h_n(x)}{\delta}-d_n(x)\right| < \epsilon\]

$f_n'$ is continuous and constant outside of a compact domain, so it's uniformly continuous.
Make $\delta$ s.t. $|x-y| < \delta \implies |f'(x)-f'(y)| <\epsilon$.

Now, for any $0 < \delta' < \delta$ we have
\begin{align*}
        & \left|\frac{h_n(x+\delta')-h_n(x)}{\delta'}-d_n(x)\right|                                           \\
  ={}   & \left|\int_{-2}^{2} g(y)\left(\frac{f_n(x-y+\delta')-f_n(x-y)}{\delta'}-f'_n(x-y)\right)\,dy\right| \\
  \le{} & \int_{-2}^{2} \left|g(y)\left(\frac{f_n(x-y+\delta')-f_n(x-y)}{\delta'}-f'_n(x-y)\right)\right|\,dy \\
  \le{} & \int_{-2}^{2} |g(y)|\left|\frac{f_n(x-y+\delta')-f_n(x-y)}{\delta'}-f'_n(x-y)\right|\,dy
\end{align*}
By the MVT, $\forall y\ \exists z \in (x-y+\delta', x-y): f'(z)=\frac{f_n(x-y+\delta')-f_n(x-y)}{\delta'}$.

$|(x-y)-z| < \delta' < \delta$, so that fraction is at most $\epsilon$ away from $f_n'(x-y)$ and
\begin{align*}
  \text{\ditto}
   & \le \int_{-2}^{2} |g(y)|\epsilon\,dy \\
   & = \epsilon \int_{-2}^{2} |g(y)|\,dy
\end{align*}
so we've got within a constant factor of $\epsilon$ with this $\delta$,
which is basically what we want. $\square$

\pagebreak

\subsubsection{And Continuously So!}

So now we know $h_n'(x)=\int_{-2}^{2} f_n'(x-y)g(y)\,dy$.

Fix an $\epsilon$, and choose $\delta$ s.t. $|a-b| < \delta \implies |f'_n(a)-f'_n(b)| < \epsilon$.

Then, for any $x$ and $x'$ s.t. $|x-x'| < \delta$,
\begin{align*}
  |h_n'(x)-h_n'(x')|
   & \le \left|\int_{-2}^{2} g(y)(f_n'(x-y)-f_n'(x'-y))\,dy\right| \\
   & \le \int_{-2}^{2} |g(y)||f_n'(x-y)-f_n'(x'-y)|                \\
   & \le \epsilon \int_{-2}^{2} |g(y)|
\end{align*}
and we have the bound we want off by just a constant factor. $\square$

\pagebreak

\subsection{Converges to \texorpdfstring{$g$}{g}}

First, notice that since $f_n$ is $0$ outside of $[-1, 1]$, for all $x \in [-1, 1]$
\[\int_{-2}^{2} f(y)\,dy = \int_{-2}^{2} f_n(x-y)\,dy\]

For any $\epsilon$, we need an $n$ big enough s.t.
\begin{align*}
  h_n(x)-g(x)
   & = \int_{-2}^{2} f_n(x-y)g(y)\,dy - g(x) \int_{-2}^{2} f_n(x-y)\,dy \\
   & = \int_{-2}^{2} f_n(x-y)(g(y)-g(x))\,dy
\end{align*}
tends to $0$ across all $x \in [-1, 1]$.

Since we're only considering $g$ on the domain $[-2, 2]$, it's uniformly continuous there.
Choose a $\delta$ s.t. $|x-y| < \delta \implies |g(x)-g(y)| < \epsilon$
and then an $n$ s.t. $|f_n(t)| < \epsilon$ when $t \notin (-\delta, \delta)$.

With these parameters chosen, when $y \le x-\delta$ or $y \ge x+\delta$,
$|x-y| \ge \delta$ and $|f_n(x-y)| < \epsilon$.

Now we can split this integral like so:
\begin{align*}
  ={} & \left|\int_{-2}^{2} f_n(x-y)(g(y)-g(x))\,dx\right|                                                                                      \\
  ={} & \int_{-2}^{x-\delta} |\text{\ditto}| \, dy+\int_{x-\delta}^{x+\delta} |\text{\ditto}| \,dy+\int_{x+\delta}^{2} |\text{\ditto}| \,dy     \\
  \le & \int_{-2}^{x-\delta} \epsilon|g(y)-g(x)| + \int_{x-\delta}^{x+\delta} |\text{\ditto}|\,dy + \int_{x+\delta}^{2} \epsilon|g(y)-g(x)|\,dy \\
  \le & \int_{-2}^{2} \epsilon|g(y)-g(x)|\,dy + \int_{x-\delta}^{x+\delta} |\text{\ditto}|\,dy                                                  \\
  \le & C\epsilon + \int_{x-\delta}^{x+\delta} |f_n(x-y)(g(y)-g(x))|\,dy \quad \text{$C$ is the bounded integral}                               \\
  \le & C\epsilon + \int_{x-\delta}^{x+\delta} |f_n(x-y)| \cdot \epsilon\,dy                                                                    \\
  \le & C\epsilon + \epsilon\int_{x-\delta}^{x+\delta} |f_n(x-y)|\,dy                                                                           \\
  \le & (C+1)\epsilon
\end{align*}
so with small enough $\delta$ and large enough $n$, the error term becomes small. $\square$

\pagebreak

\section{Problem 6}

\subsection{No Restrictions}

\subsubsection{Splitting Up the Square}

$f$ is continuous on a compact domain, so it's UC and we can choose $\delta' > 0$ s.t.
\[d((x_1, y_1), (x_2, y_2)) < \delta' \implies |f(x_1, y_1)-f(x_2, y_2)| < \epsilon\]
Then choose $n \in \N$ s.t. $\frac{1}{n} < \frac{\delta'}{\sqrt{2}}$ so
\[(x_1, y_1), (x_2, y_2) \in \left[x, x + \frac{1}{n}\right] \times \left[y, y + \frac{1}{n}\right] \implies |f(x_1, y_1)-f(x_2, y_2)| < \epsilon\]
We're basically splitting up $[0, 1] \times [0, 1]$ into a very fine $n \times n$
grid so $f$ maps any two points within the same grid cell to values that are within $\epsilon$ of each other.

\subsubsection{Making the Functions}

For convenience, now I'll make $\delta=\frac{1}{n}$.
Anyways, if $x_i=\frac{i}{n}$ where $i$ goes from $0$ to $n$, then
\begin{gather*}
  g_i(x)=\max\left(0, 1-\frac{|x-x_i|}{\delta}\right) \\
  h_i(y)=f(x_i, y)
\end{gather*}

For any $x$, $\exists i: x_i \le x \le x_{i+1}$.
For $j \ne i, i+1$ we've $\frac{|x-x_j|}{\delta} > 1$, so any other $g_j$s is irrelevant.

Only $g_i$ and $g_{i+1}$ take on nonzero values, and
\begin{align*}
  g_i(x)+g_{i+1}(x)
  &=\left(1-\frac{x-x_i}{\delta}\right)+\left(1-\frac{x_{i+1}-x}{\delta}\right) \\
  &= 2 - \frac{x_{i+1}-x_i}{\delta} = 1
\end{align*}
By construction, $|h_i(y)-f(x, y)| < \epsilon$ and $|h_{i+1}(y)-f(x, y)| < \epsilon$.

Letting $G=g_i(x)$, we have by the above equation $g_{i+1}(x)=1-G$ and
\begin{align*}
  \left|f(x, y) - \sum_{i=0}^{n} g_i(x)h_i(y)\right|
  &= |f(x, y) - Gh_i(y) - (1-G)h_{i+1}(y)| \\
  &\le |f(x, y) - h_{i+1}(y)| + G|h_{i+1}(y)-h_i(y)| \\
  &\le \epsilon + G\epsilon \\
  &\le 2\epsilon
\end{align*}
\textit{sigh}, off by a constant factor again\dots I suppose it's good enough. $\square$

\subsection{Symmetric Hell}

After thinking about it for an entire day, I'm pretty sure it isn't.

If $g_i=h_i$, then across all points $(x, x)$
\[\sum_{i=1}^{n} g_i(x)h_i(x)=\sum_{i=1}^{n} g_i(x)^2 \ge 0\]

If $f(x, y)=-1$ then it's cooked, since our approximation would never
be able to achieve negative values on the diagonal.

(if we allowed things to be complex then it should be fine tho)

\pagebreak

\section{Problem 7}

\subsection{Rational Polynomials are Dense}

Trust me, I need this later on.

Since the set of polynomials with real coefficients are dense in $C([0, 1])$,
to prove that the set of polynomials with \textit{rational} coefficients
are also dense we just need to find for everything in the former set
a uniformly convergent sequence of functions in the latter set.

So if our real-coefficient polynomial is $p(x)=\sum_{i=0}^{d} a_ix^i$
I propose that the sequence
\[p_n(x)=\sum_{i=0}^{d} R\left(a_i, \frac{1}{n}\right) x^i\]
uniformly converges to $p(x)$, where $R(a, r)$ returns any rational in $B_r(a)$.

By construction, $\left|R(a, r) - a\right| < r$, so
\begin{align*}
  |p(x)-p_n(x)|
   & = \left|\sum_{i=0}^{d} x^i\left(R\left(a_i, \frac{1}{n}\right) - a_i\right)\right| \\
   & \le \sum_{i=0}^{d} x^i \left|R\left(a_i, \frac{1}{n}\right) - a_i\right|           \\
   & \le \sum_{i=0}^{d} \frac{x^i}{n}                                                   \\
   & \le \frac{d}{n}
\end{align*}
$d$ is fixed so as $n \to \infty$ $d_\infty(p(x), p_n(x)) \to 0$.

\pagebreak

\subsection{Actual Proof}

So the set of polynomials with rational coefficients is dense in $C([0, 1])$.
This set is also countable, so we can enumerate them like $p_0, p_1, \cdots$.
For convenience, let $p_0$ be assigned so that $p_0(0)=0$.

Now we can let $u: \R \to \R$ be defined like so:
\[u(x)=\begin{cases}
    0                                             & x < 2                                 \\
    p_c(x-2c)                                     & \exists c \in \N: 2c \le x \le 2c + 1 \\
    L_{(2c + 1, p_c(1)), (2c + 2, p_{c+1}(0))}(x) & \exists c \in \N: 2c + 1 < x < 2c + 2
  \end{cases}\]
where $L_{(a, b), (c, d)}$ gives a function of the unique line that goes between $(a, b)$ and $(c, d)$.

By density, $\forall f \in C([0, 1])$ and $\epsilon > 0$, $\exists c: d_\infty(p_c, f) < \epsilon$.
Thus, by construction
\[\sup_{x \in [0, 1]} |f(x)-u(x+2c)| = \sup_{x \in [0, 1]} |f(x)-p_c(x)| < \epsilon\]

As for continuity, each piecewise part of $u$ is continuous in itself,
since everything's a polynomial.
For the integer points at $2$ or greater, the respective $p_c$
and the line segment both approach $p_c(0)$ or $p_c(1)$ depending
on if it's even or odd, so $u$ is continuous there too. $\square$

\end{document}
