\documentclass[12pt]{article}

\input{../kz}

\begin{document}
\begin{enumerate}
      \item \begin{enumerate}
                  \item We can define our linear transformation $Q: V/\text{Ker}(T) \rightarrow \text{Im}(T)$ as $Q([v])=T(v)$.
                        Let's first check that this operation is well-defined.
                        Say we have $a, b \in V: a-b \in \text{Ker}(T)$.
                        Then, \begin{align*}
                              T(b) & =T(b)+T(a-b) \\
                                   & = T(b+a-b)   \\
                                   & = T(a)
                        \end{align*}
                        So the value of $T$ does not depend on which representative of the equivalence class we choose.
                        It remains to prove that $Q$ is bijective and by extension invertible.

                        \textbf{Injective:} \\
                        If $a, b \in V: a-b \notin \text{Ker}(T)$, then $[a] \ne [b]$ as well.
                        We can go through the above chain of equalities again, only this time to prove that $T(b) \ne T(a)$.
                        Thus, $Q$ is injective.

                        \textbf{Surjective:} \\
                        Since $Q$ can map any representative of any equivalence class of $V/\text{Ker}(T)$ to $W$,
                        Anything in $\text{Im(T)}$ must be achievable by calling $Q$ on some representative of some equivalence class.
                        Thus, $Q$ is surjective as well. $\square$
                  \item Any $v \in V$ can be expressed as a unique linear combination of the elements in $\beta$.
                        Thus, we can write \begin{align*}
                              T(v) & =T\left(\sum_{i=1}^{k} a_iw_i\right) \\
                                   & =\sum_{i=1}^{k} a_iT(w_i)            \\
                                   & =\sum_{i=n+1}^{k} a_iT(w_i)
                        \end{align*}
                        Notice that the last step is valid because the elements $w_1$ to $w_n$
                        are in $\text{Ker}(T)$ and thus $T(w_i)=\vec{0}$.
                        Any $T(v)$ can be expressed in terms of a linear combination of $T(w_i)$ where $n < i \le k$.
                        Since $\text{Im}(T)=W$, this means that $\{T(w_{n+1}), \cdots, T(w_k)\}$ \textit{spans} $W$.

                        By the dimension theorem,
                        \[\dim \text{Im}(T)+\dim \text{Ker}(T)=\dim W+n=\dim V=k\]
                        $\dim W=k-n$ is the size of our spanning set, so it's a basis as well.

                        As for $[T]^\gamma_\beta$, all of $w_i$ to $w_n$ map to $\vec{0}$,
                        so their column must be all $0$s.
                        The remaining $w_i$s are equal to $\sum_{j=n+1}^{k} \mathbf{1}_{j=i} T(w_j)$,
                        so their column is all $0$s except for a $1$ in the $i$th row.

                        All in all, we'd have the identity matrix $I_{\dim W}$ except it's left-padded
                        by however many vectors of just zeros to make it match up with $\dim V$.
            \end{enumerate}
      \item \begin{enumerate}
                  \item \begin{align*}
                              (g \circ f) \circ (f^{-1} \circ g^{-1}) & = g \circ (f \circ f^{-1}) \circ g^{-1} \\
                                                                      & = g \circ 1_Y \circ g^{-1}              \\
                                                                      & = g \circ g^{-1}                        \\
                                                                      & = 1_Z                                   \\
                              (f^{-1} \circ g^{-1}) \circ (g \circ f) & = f \circ (g \circ g^{-1}) \circ f^{-1} \\
                                                                      & = f \circ 1_Z \circ f^{-1}              \\
                                                                      & = f \circ f^{-1}                        \\
                                                                      & = 1_Y\quad\square
                        \end{align*}
                  \item The base case for two functions was just proved, so let's assume $P_i$, that is,
                        \[f_n \circ f_{n-1} \circ \cdots \circ f_1: X_0 \rightarrow X_n\]
                        is invertible and has inverse
                        \[f_1^{-1} \circ f_2^{-1} \circ \cdots \circ f_n^{-1}: X_n \rightarrow X_0\]
                        It remains to prove $P_i \rightarrow P_{i+1}$.
                        For sake of brevity, let's call the above two function compositions $g$ and $g^{-1}$ resppectively.
                        Now, we just have to show that $f_{n+1} \circ g$ is invertible and has inverse $g^{-1} \circ f_{n+1}^{-1}$.
                        However, notice that we already showed that this is true in proving the previous part
                        since both $f_{n+1}$ and $g$ are invertible functions.

                        $P_i \rightarrow P_{i+1}$, so the inductive step is complete. $\square$
                  \item \begin{align*}
                              BA \cdot (A^{-1}B^{-1}) & = B \cdot (AA^{-1}) \cdot B^{-1} \\
                                                      & = B \cdot I_n \cdot B^{-1}       \\
                                                      & = BB^{-1}                        \\
                                                      & = I_n \quad\square
                        \end{align*}
            \end{enumerate}
      \item \begin{enumerate}
                  \item I presume $[T]_\beta$ means $[T]^\beta_\beta$, which in this case is
                        \[\begin{bmatrix}
                                    1 & 0 & 0 & 0 \\
                                    0 & 0 & 1 & 0 \\
                                    0 & 1 & 0 & 0 \\
                                    0 & 0 & 0 & 1 \\
                              \end{bmatrix}\]
                  \item \begin{gather*}
                              (L_A \circ f_\beta)(M)=L_A\left(\begin{bmatrix}
                                    1 \\ 2 \\ 3 \\ 4
                              \end{bmatrix}\right)=\begin{bmatrix}
                                    1 \\ 3 \\ 2 \\ 4
                              \end{bmatrix} \\
                              (f_\beta \circ T)(M)=f_\beta\left(\begin{bmatrix}
                                    1 & 3 \\
                                    2 & 4
                              \end{bmatrix}\right)=\begin{bmatrix}
                                    1 \\ 3 \\ 2 \\ 4
                              \end{bmatrix}
                        \end{gather*}
                        Yeah, these two are equal (at least for this particular matrix).
            \end{enumerate}
      \item We take the determinant along the last row first.
            \[\det \begin{bmatrix}
                        -3 & 4 & 0  & -1 \\
                        0  & 9 & -2 & -3 \\
                        1  & 1 & 1  & 1  \\
                        3  & 0 & 3  & 0
                  \end{bmatrix}=-3\det \begin{bmatrix}
                        4 & 0  & -1 \\
                        9 & -2 & -3 \\
                        1 & 1  & 1
                  \end{bmatrix}-3\det \begin{bmatrix}
                        -3 & 4 & -1 \\
                        0  & 9 & -3 \\
                        1  & 1 & 1
                  \end{bmatrix}\]
            It remains to evaluate two $3 \times 3$ determinants.
            We'll evaluate the first one along its first row, and the second one along its first column.
            \begin{align*}
                  \det \begin{bmatrix}
                             4 & 0  & -1 \\
                             9 & -2 & -3 \\
                             1 & 1  & 1
                       \end{bmatrix} & =4 \det \begin{bmatrix}
                                                     -2 & -3 \\
                                                     1  & 1
                                               \end{bmatrix}-\det \begin{bmatrix}
                                                                        9 & -2 \\
                                                                        1 & 1
                                                                  \end{bmatrix}  \\
                                       & = 4 \cdot 1 - 11                         \\
                                       & = -7                                     \\
                  \det \begin{bmatrix}
                             -3 & 4 & -1 \\
                             0  & 9 & -3 \\
                             1  & 1 & 1
                       \end{bmatrix} & =-3 \det \begin{bmatrix}
                                                      9 & -3 \\
                                                      1 & 1
                                                \end{bmatrix}+\det \begin{bmatrix}
                                                                         4 & -1 \\
                                                                         9 & -3
                                                                   \end{bmatrix} \\
                                       & = -3 \cdot 12 - 3                        \\
                                       & = -39
            \end{align*}
            Combining these two, we get that the determinant is $-3 \cdot (-7) + -3 \cdot (-39)=\boxed{138}$.
      \item \begin{enumerate}
                  \item We'll choose the points $\left(1, -\frac{1}{m}\right)$ and $(1, m)$ respectively.
                        The first point maps to $\left(-1, \frac{1}{m}\right)$ by geometry and the second one maps to itself as it's on the line.
                        These two points are also \textit{not} scalar multiples of each other, so we can construct $[T]^\gamma_\beta$ from this.

                        To express any $(x,y)$ in terms of a linear combo of these two points, we must solve
                        \begin{gather*}
                              x=a+b \\
                              y=-\frac{a}{m}+bm
                        \end{gather*}
                        first, which gives us
                        \[(x,y) \rightarrow \frac{m^2x-my}{m^2+1}\left(1, -\frac{1}{m}\right)+\frac{x+my}{m^2+1}(1, m)\]
                        Finally, to reflect this across $y=mx$, we can multiply the first coefficient by $-1$ and leave the second one unchanged.
                        This gives us our result of
                        \begin{align*}
                              T((x,y)) & =\frac{my-m^2x}{m^2+1}\left(1, -\frac{1}{m}\right)+\frac{x+my}{m^2+1}(1, m) \\
                                       & = \boxed{\left(\frac{2my-m^2x+x}{m^2+1}, \frac{2mx+m^2y-y}{m^2+1}\right)}
                        \end{align*}
                  \item I know the \textit{intended} solution is with bases and whatnot,
                        but I already solved this with coordinate geometry, so...

                        Any line perpendicular to $L$ must have slope $m$.
                        If our initial point is $(x_0, y_0)$, then the line that is perpendicular to $L$ and goes through the initial point must be
                        \begin{gather*}
                              m=\frac{y-y_0}{x-x_0} \\
                              y=m(x_0-x)+y_0
                        \end{gather*}
                        The projection is there this line and $y=-\frac{1}{m}x$ meet.
                        Solving, we get that the projection of $(x_0, y_0)$ and $L$ is
                        \[\left(\frac{m^2x_0+my_0}{m^2-1}, \frac{y_0+mx_0}{1-m^2}\right)\]
            \end{enumerate}
      \item \begin{enumerate}
                  \item We calculate the effects of $T$ on all of the elements in the bases.
                        \begin{gather*}
                              T(E_{11})=E_{11}+E_{22} \\
                              T(E_{12})=E_{12} \\
                              T(E_{21})=\vec{0} \\
                              T(E_{22})=\vec{0} \\
                              T(E_{11}+E_{22})=E_{11}+E_{22}
                        \end{gather*}
                        Putting the results of these in matrices, we get
                        \begin{gather*}
                              [T]_S=\begin{bmatrix}
                                    1 & 0 & 0 & 0 \\
                                    0 & 1 & 0 & 0 \\
                                    0 & 0 & 0 & 0 \\
                                    1 & 0 & 0 & 0
                              \end{bmatrix} \\
                              [T]_\beta=\begin{bmatrix}
                                    0 & 0 & 0 & 0 \\
                                    1 & 1 & 0 & 0 \\
                                    0 & 0 & 1 & 0 \\
                                    0 & 0 & 0 & 0
                              \end{bmatrix}
                        \end{gather*}
                  \item $[I_V]^\beta_S$ is the elements in $S$ re-expressed in terms of the elements of $\beta$, as follows:
                        \begin{gather*}
                              E_{11}=E_{11} \\
                              E_{12}=E_{12} \\
                              E_{21}=E_{21} \\
                              E_{22}=(E_{11}+E_{22})-E_{11} \\
                              Q=[I_V]^\beta_S=\begin{bmatrix}
                                    1 & 0 & 0 & -1 \\
                                    0 & 0 & 0 & 1  \\
                                    0 & 1 & 0 & 0  \\
                                    0 & 0 & 1 & 0
                              \end{bmatrix} \\
                              Q^{-1}=\begin{bmatrix}
                                    1 & 1 & 0 & 0 \\
                                    0 & 0 & 1 & 0 \\
                                    0 & 0 & 0 & 1 \\
                                    0 & 1 & 0 & 0
                              \end{bmatrix}
                        \end{gather*}
                  \item Evaluating $Q^{-1} \cdot [T]_\beta \cdot Q$, we see that the result is indeed equal to $[T]_S$.
                        \begin{gather*}
                              Q^{-1} \cdot [T]_\beta = \begin{bmatrix}
                                    1 & 1 & 0 & 0 \\
                                    0 & 0 & 1 & 0 \\
                                    0 & 0 & 0 & 0 \\
                                    1 & 1 & 0 & 0
                              \end{bmatrix} \\
                              \left(Q^{-1} \cdot [T]_\beta\right) \cdot Q = \begin{bmatrix}
                                    1 & 0 & 0 & 0 \\
                                    0 & 1 & 0 & 0 \\
                                    0 & 0 & 0 & 0 \\
                                    1 & 0 & 0 & 0
                              \end{bmatrix}
                        \end{gather*}
            \end{enumerate}
      \item \begin{enumerate}
                  \item To get $[3, 1, 2]$ from $[1, 2, 3]$, we can swap the elements like so:
                        \begin{align*}
                              [1, 2, 3] & \rightarrow [3, 2, 1] \\
                                        & \rightarrow [3, 1, 2]
                        \end{align*}
                        It took an even number of swaps, so $\text{sign}(\sigma)=\boxed{0}$.
                  \item Any $\sigma$ can be expressed as follows:
                        \[\sigma = \sigma_{i_k, j_k} \circ \sigma_{i_{k-1}, j_{k-1}} \circ \cdots \circ \sigma_{i_1, j_1}\]
                        The inverse of each of these swaps is another swap, so by the result in problem 2,
                        $\sigma$ is invertible, with its inverse being
                        \[\sigma_{i_1, j_1}^{-1} \circ \sigma_{i_2, j_2}^{-1} \circ \cdots \circ \sigma_{i_k, j_k}^{-1}\]
                        Since the number of swaps used to get $\sigma^{-1}$ is equal to the number of swaps used for $\sigma$,
                        $\text{sign}\left(\sigma^{-1}\right)=\text{sign}(\sigma)$. $\square$
                  \item $\sigma \circ \tau$ can be expressed as the composition of all of their swaps as well.
                        Because of this, if $\sigma$ takes $n$ swaps and $\tau$ takes $m$ swaps, then we
                        can achieve $\sigma \circ \tau$ with $n+m$ swaps.

                        Taking this number mod 2, it's equal to $n$ mod 2 plus $m$ mod 2, which is equal to $\text{sign}(\sigma)+\text{sign}(\tau)$.
                        Thus, $\text{sign}(\sigma \circ \tau)=\text{sign}(\sigma)+\text{sign}(\tau)$. $\square$
                  \item First, we're going to prove that $\exists \delta$ s.t.
                        \[\{A_{1\sigma(1)}, A_{2\sigma(2)}, \cdots, A_{n\sigma(n)}\}=
                              \{A_{\delta(1)1}, A_{\delta(2)2}, \cdots, A_{\delta(n)n}\}\]
                        We can just treat the positions as their own ordered pairs, so this reduces to proving that
                        \[\{(1, \sigma(1)), (2, \sigma(2)), \cdots, (n, \sigma(n))\}=
                              \{(\delta(1), 1), (\delta(2), 2), \cdots, (\delta(n), n)\}\]
                        Notice that to prove this, we can prove that
                        \[\exists \delta: \delta(\sigma(i))=i\ \forall 1 \le i \le n\]
                        Consider $\delta=\sigma^{-1}$.
                        All permutations are invertible, so we can invert $\sigma$.
                        Then, $\delta(\sigma(i))=i$ and the two sets above are equivalent.

                        By the result in a previous part, $\sigma$ and $\sigma^{-1}$ have the same sign.

                        Also, the function $f: S_n \rightarrow S_n$ defined by $f(\sigma)=\sigma^{-1}$ is a bijection,
                        since all inverses are unique.

                        Finally, given all this, we can rewrite the summation as
                        \begin{align*}
                              \sum_{\sigma \in S_n} (-1)^{\text{sign}(\sigma)} A_{1\sigma(1)} \cdots A_{n\sigma(n)}
                               & = \sum_{\sigma \in S_n} (-1)^{\text{sign}\left(\sigma^{-1}\right)} A_{\sigma^{-1}(1)1} \cdots A_{\sigma^{-1}(n)n} \\
                               & =\sum_{\sigma \in S_n} (-1)^{\text{sign}(\sigma)} A_{\sigma(1)1} \cdots A_{\sigma(n)n}                            \\
                               & =\sum_{\sigma \in S_n} (-1)^{\text{sign}(\sigma)} A^T_{1\sigma(1)} \cdots A^T_{n\sigma(n)}
                        \end{align*}
                        As we can see, the last expression is equivalent to $\det A^T$. $\square$
                  \item \textbf{Base case:}
                        \[\det A=(-1)^0 \cdot A_{11}A_{22}+(-1)^1 \cdot A_{12}A_{21}\]
                        The first term corresponds to the permutation $[1, 2]$, while the second corresponds to $[2, 1]$.
                        Thus, the LHS and RHS are equal for a $2 \times 2$ matrix.

                        We now assume that the two determinant formulas are equivalent for $n \times n$ matrices
                        and try to prove that they work for $n+1 \times n+1$ matrices.

                        \textbf{Inductive step:} \\
                        Let's take $K_n(i)$ as the subset of $S_n$ that contains
                        all the permutations of $S_n$ that have a $1$ at position $i$.

                        WLOG assume $j=1$, since the determinant does not matter with regards to where we choose our column to take our product.
                        We propose that
                        \[(-1)^{i+1}A_{i1} \det \overline{A_{i1}}=\sum_{\sigma \in K_n(i)} (-1)^{\text{sign}(\sigma)} A_{1\sigma(1)} \cdots A_{n\sigma(n)}\]

                        Let's define $T: S_n \rightarrow S_{n-1}$ which
                        takes in a $\sigma$ and returns a $\sigma'$ defined by the piecewise function
                        \[\sigma'(x)\begin{cases}
                                    \sigma(x)-1\quad x < i \\
                                    \sigma(x+1)-1\quad x \ge i
                              \end{cases}\]
                        Notice that $\text{Im}(T)=S_{n-1}$, since the function is injective and the size of the two sets are both $(n-1)!$.

                        We can then simplify the RHS, since $\sigma(i)=1\ \forall \sigma \in K_n(i)$.
                        \begin{align*}
                               & \hphantom{=} \sum_{\sigma \in K_n(i)} (-1)^{\text{sign}(\sigma)} A_{1\sigma(1)} \cdots A_{n\sigma(n)}                                                         \\
                               & =A_{i1}\sum_{\sigma \in K_n(i)} (-1)^{\text{sign}(\sigma)} A_{1\sigma(1)} \cdots A_{i-1\sigma(i-1)}A_{i+1\sigma(i+1)}  \cdots A_{n\sigma(n)}                  \\
                               & =A_{i1}\sum_{\sigma \in K_n(i), \delta=T(\sigma)} (-1)^{\text{sign}(\sigma)} A_{1\delta(1)} \cdots A_{i-1\delta(i-1)}A_{i+1\sigma(i)} \cdots A_{n\delta(n-1)}
                        \end{align*}

                        To transition fully to the terms in $S_{n-1}$, we must somehow relate
                        the sign of $\sigma$ to the sign of $\delta$.
                        To do this, we use the fact that the sign of a permutation is equal to the number of inversions in it mod 2.
                        Regardless of how many inversions $\delta$ has,
                        turning it back into $\sigma$ going to add $i-1$ inversions to it.

                        Thus, $\text{sign}(\sigma)=\text{sign}(\delta)+i-1$ and we can continue simplifying the expression like so:
                        \begin{align*}
                               & \hphantom{=} A_{i1}\sum_{\sigma \in K_n(i), \delta=T(\sigma)} (-1)^{\text{sign}(\sigma)} A_{1\delta(1)} \cdots A_{n\delta(n-1)}  \\
                               & = A_{i1} (-1)^{i-1} \sum_{\sigma \in K_n(i), \delta=T(\sigma)} (-1)^{\text{sign}(\delta)} A_{1\delta(1)} \cdots A_{n\delta(n-1)} \\
                               & = A_{i1} (-1)^{i-1} \sum_{\delta \in S_{n-1}} (-1)^{\text{sign}(\delta)} A_{1\delta(1)} \cdots A_{n\delta(n-1)}                  \\
                               & = A_{i1}(-1)^{i-1} \det \overline{A_{i1}}
                        \end{align*}
                        We've finally rewrote the RHS to be equal to the LHS, and the inductive step is complete. $\square$
            \end{enumerate}
\end{enumerate}
\end{document}
