\documentclass[12pt]{article}

\input{../kz}

\allowdisplaybreaks

\begin{document}
\begin{enumerate}
      \item \begin{gather*}
                  \begin{aligned}
                        \braket{x,y} & =2\cdot\overline{2+i}+(1+i) \cdot 2+i\cdot\overline{1+2i} \\
                                     & =4+2i+2+2i+i+2                                            \\
                                     & =8+5i
                  \end{aligned} \\
                  \begin{aligned}
                        ||x|| & = \sqrt{\braket{x, x}}                                          \\
                              & =\sqrt{2 \cdot 2 +  (1+i)(\overline{1+i})+i \cdot \overline{i}} \\
                              & =\sqrt{7}
                  \end{aligned} \\
                  \begin{aligned}
                        ||y|| & =  \sqrt{(2-i)(\overline{2-i})+2 \cdot 2+(1+2i)(\overline{1+2i})} \\
                              & =\sqrt{14}
                  \end{aligned} \\
                  \begin{aligned}
                        ||x+y|| & = \sqrt{(4-i)(\overline{4-i}) + (3+i)(\overline{3+i}) + (1+3i)(\overline{1+3i})} \\
                                & = \sqrt{37}
                  \end{aligned} \\
                  |\braket{x,y}|=\sqrt{89} \le \sqrt{7} \cdot \sqrt{14} = ||x|| \cdot ||y||\quad\text{Cauchy-Schwarz satisfied} \\
                  ||x+y||=\sqrt{37} \le \sqrt{7}+\sqrt{14}=||x||+||y||\quad\text{Triangle satisfied}
            \end{gather*}
      \item \begin{enumerate}
                  \item $\mathbf{\braket{cx, y}=c\braket{x, y}}$:
                        \begin{align*}
                              \braket{cA, B} & = \text{tr}(B^* (cA))      \\
                                             & = \text{tr}(c \cdot B^* A) \\
                                             & = c \cdot \text{tr}(B^* A) \\
                                             & = c\braket{A, B}
                        \end{align*}

                        $\mathbf{\overline{\braket{x, y}}=\braket{y, x}}$:
                        \begin{align*}
                              \overline{\braket{A, B}} & = \overline{\text{tr}(B^*A)}                                   \\
                                                       & = \overline{\sum_{i=1}^{n} \left(B^*A\right)_{ii}}             \\
                                                       & = \sum_{i=1}^{n} \overline{\left(B^*A\right)_{ii}}             \\
                                                       & = \sum_{i=1}^{n} \overline{\sum_{j=1}^{n} B^*_{ij}A_{ji}}      \\
                                                       & = \sum_{i=1}^{n} \sum_{j=1}^{n} \overline{B^*_{ij}A_{ji}}      \\
                                                       & = \sum_{i=1}^{n} \sum_{j=1}^{n} B_{ji} \cdot \overline{A_{ji}} \\
                                                       & = \sum_{i=1}^{n} \sum_{j=1}^{n} B_{ji} \cdot A^*_{ij}          \\
                                                       & = \sum_{i=1}^{n} \left(A^*B\right)_{ii}                        \\
                                                       & = \braket{B, A}\quad\square
                        \end{align*}
                  \item \begin{gather*}
                              \begin{aligned}
                                    A^*=\begin{bmatrix}
                                              1   & 3  \\
                                              2-i & -i
                                        \end{bmatrix} &  &
                                    B^*=\begin{bmatrix}
                                              1-i & -i \\
                                              0   & i
                                        \end{bmatrix}
                              \end{aligned} \\
                              \begin{aligned}
                                    ||A|| & =\sqrt{\text{tr}\left(A^*A\right)}                                              \\
                                          & =\sqrt{\text{tr}\left(\begin{bmatrix}10 & 2+4i \\ 2-4i & 6\end{bmatrix}\right)} \\
                                          & = 4
                              \end{aligned} \\
                              \begin{aligned}
                                    ||B|| & = \sqrt{\text{tr}\left(B^*B\right)}                                         \\
                                          & = \sqrt{\text{tr}\left(\begin{bmatrix}3 & -1 \\ -1 & 1\end{bmatrix}\right)} \\
                                          & = 2
                              \end{aligned} \\
                              \begin{aligned}
                                    \braket{A, B} & = \text{tr}\left(B^* A\right)         \\
                                                  & = \text{tr}\left(\begin{bmatrix}
                                                                                 1-4i & 4-i \\
                                                                                 3i   & -1
                                                                           \end{bmatrix}\right) \\
                                                  & = -4i
                              \end{aligned}
                        \end{gather*}
            \end{enumerate}
      \item \begin{enumerate}
                  \item Take $x=(1, 3)$.
                        Although it's nonzero, $\braket{x, x}=1 \cdot 1 - 3 \cdot 3 < 0$, even though it should be positive. \label{list:3a}
                  \item Take $A=-I_2$.
                        Similarly to \ref{list:3a}, though it's nonzero, $\braket{A, A}=\text{tr}(-2I_2)=-2 < 0$.
                  \item Take $f(x)=e^{-x}$.
                        Even though it's nonzero, we get that
                        \begin{align*}
                              \braket{f(x), f(x)} & = \int f'(x)f(x)\,dx                                \\
                                                  & = \int -e^{-x} \cdot e^{-x}\,dx                     \\
                                                  & =-\int e^{-2x}\,dx                                  \\
                                                  & =-\left.\left(-\frac{1}{2}e^{-2x}\right)\right|^1_0 \\
                                                  & =\frac{1}{2}e^{-2}-\frac{1}{2}                      \\
                                                  & < 0
                        \end{align*}

                        \small{lol the last axiom solos all of these definitions}
            \end{enumerate}
      \item \begin{enumerate}
                  \item $x$ can be written as a linear combo of the elements in $\beta$.
                        Thus, we have
                        \begin{align*}
                              \braket{x, x} & = \Braket{\sum_{i=1}^{n} a_i \beta_i, x} \\
                                            & = \sum_{i=1}^{n} \braket{a_i \beta_i, x} \\
                                            & = \sum_{i=1}^{n} a_i \braket{\beta_i, x} \\
                                            & = 0
                        \end{align*}
                        Since $\braket{x, x}=0$ iff $x=0$, $x=0$. $\square$ \label{list:4a}
                  \item \begin{align*}
                              \braket{x, z}=\braket{y, z} & \leftrightarrow \braket{x, z}-\braket{y, z}=0 \\
                                                          & \leftrightarrow \braket{x-y, z}= 0            \\
                                                          & \leftrightarrow x-y=0                         \\
                                                          & \leftrightarrow x=y\quad\square
                        \end{align*}
                        The second-to-last link is due to what we just proved in \ref{list:4a}
            \end{enumerate}
      \item \begin{enumerate}
                  \item Let the first element in the basis be $(1, 0, 1)$.
                        The square of its norm is $1^2+1^2=2$.
                        Then, the second element is
                        \begin{align*}
                              (0, 1, 1) - p_{U_1}((0, 1, 1)) & = (0, 1, 1) - \frac{(0, 1, 1) \cdot (1, 0, 1)}{2}(1, 0, 1) \\
                                                             & = (0, 1, 1) - \left(\frac{1}{2}, 0, \frac{1}{2}\right)     \\
                                                             & = \left(-\frac{1}{2}, 1, \frac{1}{2}\right)
                        \end{align*}
                        and the square of its norm is $\frac{1}{2}^2+1^2+\frac{1}{2}^2=\frac{3}{2}$.

                        Moving on to the third element, we get its transformed version
                        \begin{align*}
                              (1, 3, 3) - p_{U_2}((1, 3, 3)) & = (1, 3, 3) - \frac{(1, 3, 3) \cdot t_1}{2}t_1 - \frac{(1, 3, 3) \cdot t_2}{\frac{3}{2}}t_2 \\
                                                             & = (1, 3, 3) - (2, 0, 2) - \left(-\frac{4}{3}, \frac{8}{3}, \frac{4}{3}\right)               \\
                                                             & = \left(\frac{1}{3}, \frac{1}{3}, -\frac{1}{3}\right)
                        \end{align*}

                        Normalizing these vectors, we get our orthonormal basis
                        \[\left\{\left(\frac{1}{\sqrt{2}}, 0, \frac{1}{\sqrt{2}}\right),
                              \left(-\frac{1}{\sqrt{6}}, \sqrt{\frac{2}{3}}, \frac{1}{\sqrt{6}}\right),
                              \left(\frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}}, -\frac{1}{\sqrt{3}}\right)\right\}\]
                        The Fourier coefficients of $(1, 1, 2)$ relative to this basis are
                        \begin{gather*}
                              \Braket{(1, 1, 2), \left(\frac{1}{\sqrt{2}}, 0, \frac{1}{\sqrt{2}}\right)}=\frac{3}{\sqrt{2}} \\
                              \Braket{(1, 1, 2), \left(-\frac{1}{\sqrt{6}}, \sqrt{\frac{2}{3}}, \frac{1}{\sqrt{6}}\right)}=\frac{3}{\sqrt{6}} \\
                              \Braket{(1, 1, 2), \left(\frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}}, -\frac{1}{\sqrt{3}}\right)}=0
                        \end{gather*}
                        Summing them up, we get
                        \[\left(\frac{3}{2}, 0, \frac{3}{2}\right)+\left(-\frac{3}{6}, 1, \frac{3}{6}\right)=(1, 1, 2)\]
                        which is our original vector.

                        \setcounter{enumii}{2}
                  \item Computations of integrals are omitted for brevity.

                        The first element is $1$, and the square of its norm is $1$.

                        As for the second element, it'S
                        \begin{align*}
                              x-p_{U_1}(x) & = x-\frac{\int_0^1 x \cdot 1\,dx}{1}1 \\
                                           & = x-\frac{1}{2}
                        \end{align*}
                        and the square of its norm is $\frac{1}{12}$.

                        For the final element, we have it as
                        \begin{align*}
                              x^2-p_{U_2}(x^2) & = x^2-\frac{\int_0^1 x^2\cdot 1\,dx}{1}1-
                              \frac{\int_0^1 x^2\left(x-\frac{1}{2}\right)\,dx}{\frac{1}{12}}\left(x-\frac{1}{2}\right) \\
                                               & = x^2-\frac{1}{3}-\left(x-\frac{1}{2}\right)                           \\
                                               & = x^2-x+\frac{1}{6}
                        \end{align*}
                        and the square of its norm is $\frac{1}{180}$.

                        Normalizing these functions, we get an orthonormal basis of
                        \[\left\{1, 2\sqrt{3}x-\sqrt{3}, 6\sqrt{5}(x^2-x)+\sqrt{5}\right\}\]

                        The Fourier coefficients of $x+1$ relative to this basis are
                        \begin{gather*}
                              \int_0^1 (x+1) \cdot 1\,dx=\frac{3}{2} \\
                              \int_0^1 (x+1) \cdot \left(2\sqrt{3}x-\sqrt{3}\right)\,dx=\frac{1}{2\sqrt{3}} \\
                              \int_0^1 (x+1) \cdot \left(6\sqrt{5}(x^2-x)+\sqrt{5}\right)\,dx=0
                        \end{gather*}
                        Combining them with the elements of the basis, we get
                        \[\frac{3}{2}+\frac{2\sqrt{3}x}{2\sqrt{3}}-\frac{1}{2}=x+1\]
                        which is our original function.
                  \item Same thing as the previous two problems.
                        The first element is $(1, i, 0)$, and its norm squared is $1^2+i(-i)=2$.

                        The other element is
                        \begin{align*}
                              (1-i, 2, 4i) - p_{U_1}((1-i, 2, 4i)) & =(1-i, 2, 4i) - \frac{(1-i, 2, 4i) \cdot (1, i, 0)}{2}(1, i, 0) \\
                                                                   & = (1-i, 2, 4i)-\left(\frac{1-3i}{2}, \frac{3+i}{2}, 0\right)    \\
                                                                   & = \left(\frac{1+i}{2},\frac{1-i}{2},4i\right)
                        \end{align*}

                        Normalizing, we get
                        \[\left\{\left(\frac{1}{\sqrt{2}}, \frac{i}{\sqrt{2}}, 0\right),
                        \left(\frac{1+i}{2\sqrt{17}}, \frac{1-i}{2\sqrt{17}}, \frac{4i}{\sqrt{17}}\right)\right\}\]

                        The Fourier coefficients of $(3+i, 4i, -4)$ relative to this basis are
                        \begin{gather*}
                              \Braket{(3+i, 4i, -4), \left(\frac{1}{\sqrt{2}}, \frac{i}{\sqrt{2}}, 0\right)}=\frac{7+i}{\sqrt{2}} \\
                              \Braket{(3+i, 4i, -4), \left(\frac{1+i}{2\sqrt{17}}, \frac{1-i}{2\sqrt{17}}, \frac{4i}{\sqrt{17}}\right)}=i\sqrt{17}
                        \end{gather*}
                        Doing the same thing as always, we sum them up to get
                        \[\left(\frac{7+i}{2}, \frac{7i-1}{2}, 0\right)+\left(\frac{i-1}{2}, \frac{i+1}{2}, -4\right)=(3+i, 4i, -4)\]
                        our original vector.
            \end{enumerate}
      \item For a vector to be orthogonal to the two given in $S$, has to satisfy the following equations:
            \begin{gather*}
                  a-ci=0 \\
                  a+2b+c=0
            \end{gather*}
            Taking $c$ as the independent term, we find that $a=ci$ and $b=-\frac{1+i}{2}c$.

            Thus, $S^\perp=\boxed{\text{span}\left(\left\{\left(i, -\frac{1+i}{2}, 1\right)\right\}\right)}$.
      \item \begin{enumerate}
                  \item \textbf{Forward Direction:} \\
                        If $w \in U^\perp$, then its inner product with all the elements in $U$ is $0$.
                        By extension, its inner product with any linear combination of the elements is also $0$ because
                        \begin{align*}
                              \braket{w, u} & = \Braket{w, \sum_{i=1}^{n} a_i u_i}            \\
                                            & = \overline{\Braket{\sum_{i=1}^{n} a_i u_i, w}} \\
                                            & = \sum_{i=1}^{n} a_i \overline{\braket{u_i, w}} \\
                                            & = 0
                        \end{align*}
                        As the span of $U$ is defined by all the elements that can be formed with a linear combo
                        of the elements in $U$, we can see that $\braket{w, u}=0\ \forall u \in S$.

                        \textbf{Backward Direction:} \\
                        Any set is contained in its own span.

                        Since $\braket{w, u}=0\ \forall u \in \text{span}(U)$,
                        $\braket{w, u}=0\ \forall u \in U$ as well and thus $w \in U^\perp$. $\square$
                  \item We'll use the old definition of direct sum, that is,
                        that a sum of subspaces is a direct sum if their intersection is $\{\vec{0}\}$.

                        $\vec{0}$ is in $U$ since it's a subspace, and it's also
                        in $U^\perp$ because $\braket{\vec{0}, x}=0\ \forall x \in V$.

                        To prove that \textit{only} $\vec{0}$ is in the intersection,
                        SWOC that there's a nonzero vector $v$ in both $U$ and $U^\perp$.
                        This means that $\braket{v, u}=0\ \forall u \in U$.
                        However, since $v \in U^\perp$, $\braket{v, v}=0$, which is true iff $v \ne \vec{0}$.
                        Contradiction.
                        Thus, $U+U^\perp$ is a direct sum. $\square$ \label{list:7b}

                  \item $U$ always has a finite, orthonormal basis because
                        the Gram-Schmidt Process always generates a finite, orthogonal basis
                        and we can normalize the vectors in that basis to make it orthonormal.

                  \item We've already proved that the sum is direct.
                        Let $v=p_U(v)+(v-p_U(v))$.
                        We've proven in class that $p_U(v) \in U$ and $v-p_U(v) \in U^\perp$,
                        so any $v \in V$ can be written as the sum of an
                        element from $U$ and another from $U^\perp$. $\square$

                  \item \textbf{Existing $\rightarrow$ New:} \\
                        Again, it was proven in class that according to our original
                        definition of projection, $v-p_U(v) \in U^\perp$.
                        Thus, we can choose $x=p_U(v)$ and $z=v-p_U(v)$,
                        as these two satisfy the definition and make it in line
                        with our existing one.

                        \textbf{New $\rightarrow$ Existing:} \\
                        Let $v=x+z$ for $x \in U$ and $z \in U^\perp$.
                        By \ref{list:7b}, $x$ and $z$ are unique.

                        Let the $p_U$ of the old definition now be refered to as $p_U'$.
                        Since $v-p_U'(v) \in U^\perp$, $p_U'(v) \in U$, and their sum is $v$,
                        we can choose $p_U'(v)$ as our $x$.
                        And because $x$ is unique, it forces $x=p_U(v)=p_U'(v)$. $\square$
            \end{enumerate}
      \item We've previously proven that $x=z+y$ where $z \in W$ and $y \in W^\perp$.
            Now, notice that
            \begin{align*}
                  \braket{x, y} & = \braket{z+y, y}               \\
                                & = \braket{z, y} + \braket{y, y} \\
                                & = \braket{y, y}
            \end{align*}
            The last equality is because $z$ and $y$ are orthogonal to each other.

            $\braket{y, y} \ne 0 \leftrightarrow y \ne 0$, so it remains to prove
            that $y \ne 0$.
            Since $x \notin W$, $x-z=y \ne 0$ and by extension $\braket{x, y} \ne 0$.
            $\square$ \label{list:8}
      \item \begin{enumerate}
                  \item Take $s \in S^\perp$.
                        Since $\braket{s, x}=0\ \forall x \in S$ and $S_0 \subseteq S$,
                        $\braket{s, x}=0\ \forall x \in S_0$ and
                        by extension $s \in S_0^\perp$ as well. $\square$
                  \item Take $s \in S$.
                        By the definition of $S^\perp$, $\braket{s, x}=0\ \forall x \in S^\perp \therefore s \in (S^\perp)^\perp$.

                        By the definition of span, let's take any linear combo
                        of elements in $S$ $\sum_{i=1}^{n} a_i s_i$.
                        $\braket{s, x}=0\ \forall s \in S, x \in S^\perp$,
                        so we have
                        \[\Braket{\sum_{i=1}^{n} a_i s_i, x}=0\ \forall x \in S^\perp\]
                        as well.
                        This means that any $s \in \text{span}(S)$ must be orthogonal
                        to all the elements in $S^\perp$ and by extension $s \in (S^\perp)^\perp$. $\square$
                  \item We've proven $W \subseteq (W^\perp)^\perp$ already,
                        so now we have to prove $(W^\perp)^\perp \subseteq W$.

                        Take $w \in (W^\perp)^\perp$.
                        BWOC say $w \notin W$.
                        Then, by \ref{list:8}, $\exists x \in W^\perp: \braket{w, x} \ne 0$.
                        However, since $w \in (W^\perp)^\perp$, $\braket{w, x}=0\ \forall x \in W^\perp$
                        by definition.
                        Contradiction.
                        Thus, $w \in W$, $(W^\perp)^\perp \subseteq W$, and
                        $(W^\perp)^\perp = W$. $\square$
            \end{enumerate}
      \item $\mathbf{(W_1+W_2)^\perp \subseteq W_1^\perp \cap W_2^\perp}$ \\
            Suppose $x \in (W_1+W_2)^\perp$.
            $\braket{w_1+w_2, x}=0\ \forall w_1 \in W_1, w_2 \in W_2$.
            Since $W_1$ is a subspace, we can take $w_1=\vec{0}$ to see that
            $\braket{w_1+w_2, x}=\braket{w_2, x}=0 \therefore x \in W_2^\perp$.

            By a symmetrical argument, we can also deduce that $x \in W_1^\perp$.
            Thus, $x \in W_1^\perp \cap W_2^\perp$.

            $\mathbf{(W_1+W_2)^\perp \supseteq W_1^\perp \cap W_2^\perp}$
            \begin{align*}
                  w \in W_1^\perp \cap W_2^\perp & \therefore \braket{w, w_1}=\braket{w, w_2}=0\ \forall w_1 \in W_1, w_2 \in W_2 \\
                                                 & \therefore \braket{w, w_1}+\braket{w, w_2}=0                                   \\
                                                 & \therefore \braket{w, w_1+w_2}=0                                               \\
                                                 & \therefore \braket{w, x}=0\ \forall x \in W_1+W_2                              \\
                                                 & \therefore w \in (W_1+W_2)^\perp
            \end{align*}

            Since each set is contained in the other, the two are equal. $\square$
      \item \begin{enumerate}
                  \item I suppose there's no choice other than to go through the axioms one by one.
                        \begin{enumerate}
                              \item $\mathbf{\braket{x+z, y}=\braket{x,y}+\braket{z, y}}$
                                    \begin{align*}
                                          \braket{x+z, y} & = \sum_{n=1}^{\infty} (x+z)(n)\overline{y(n)}                                     \\
                                                          & = \sum_{n=1}^{\infty} x(n)\overline{y(n)}+z(n)\overline{y(n)}                     \\
                                                          & = \sum_{n=1}^{\infty} x(n)\overline{y(n)}+\sum_{n=1}^{\infty} z(n)\overline{y(n)} \\
                                                          & = \braket{x, y}+\braket{z, y}
                                    \end{align*}
                              \item $\mathbf{\braket{cx, y}=c\braket{x, y}}$
                                    \begin{align*}
                                          \braket{cx, y} & = \sum_{n=1}^{\infty} (cx)(n)\overline{y(n)}      \\
                                                         & = \sum_{n=1}^{\infty} c \cdot x(n)\overline{y(n)} \\
                                                         & = c\sum_{n=1}^{\infty} x(n)\overline{y(n)}        \\
                                                         & = c\braket{x, y}
                                    \end{align*}
                              \item $\mathbf{\overline{\braket{x, y}}=\braket{y, x}}$
                                    \begin{align*}
                                          \overline{\braket{x, y}} & = \overline{\sum_{n=1}^{\infty} x(n)\overline{y(n)}}                   \\
                                                                   & = \sum_{n=1}^{\infty} \overline{x(n)} \cdot \overline{\overline{y(n)}} \\
                                                                   & = \sum_{n=1}^{\infty} y(n)\overline{x(n)}                              \\
                                                                   & =\braket{y, x}
                                    \end{align*}
                              \item $\mathbf{x \ne 0 \leftrightarrow \braket{x, x}>0}$ \\
                                    \textbf{Forward Direction:} \\
                                    If $x \ne 0$, $\exists n: x(n) \ne 0$.
                                    Also, as previously established in a worksheet, a nonzero complex
                                    number times its conjugate is a positive real number, so
                                    $x(n)\overline{x(n)} \ne 0\ \forall n: x(n) \ne 0$.
                                    
                                    With these two facts, we can conclude that
                                    $\sum_{n=1}^{\infty} x(n) \overline{x(n)} \ne 0$.

                                    \textbf{Backward Direction:} \\
                                    For any $x$, $\sum_{n=1}^{\infty} x(n)\overline{x(n)}$ is nonnegative.
                                    If $\braket{x, x}>0$, then $\exists n: x(n) \ne 0$ and by extension $x \ne 0$.
                        \end{enumerate}
                        All the axioms are fulfilled, and thus
                        $\braket{\cdot, \cdot}$ is an inner product on $V$. $\square$
                  \item For any $e_n$,
                        \begin{align*}
                              \braket{e_n, e_n} & =\sum_{i=1}^{\infty} \delta_{ni}\overline{\delta_{ni}} \\
                                                & = \delta_{nn}\overline{\delta_{nn}}                    \\
                                                & = 1
                        \end{align*}
                        so the set is normal.

                        Also, for any distinct pair $n$ and $n'$,
                        \begin{align*}
                              \braket{e_n, e_{n'}} & = \sum_{i=1}^{\infty} \delta_{ni}\overline{\delta_{n'i}}                   \\
                                                   & = \delta_{nn}\overline{\delta_{n'n}} +\delta_{nn'}\overline{\delta_{n'n'}} \\
                                                   & = 0
                        \end{align*}
                        so the set is orthogonal as well.

                        To prove LI, we set up the equation
                        \[\sum_{i=1}^{\infty} a_i e_i=0\]
                        For this equality to hold, all $a_i$s must be $0$ since
                        they're the only one contributing to the $i$th term of the sequence.
                        Since the only solution to this equation is $a_i=0\ \forall i$,
                        the set is LI.

                        Notice that this set also spans $V$, since any sequence $x$ can be written as
                        \[x=\sum_{n=1}^{\infty} x(n)e_n\]

                        With these properties, we can finally say that this set is an orthonormal basis. $\square$
                  \item \begin{enumerate}
                              \item For $e_1$ to be in $W$, we would need
                                    it to be a linear combo of the elements in the given set.
                                    In other words,
                                    \[e_1 = \sum_{i=2}^{\infty} a_i(e_1+e_i)\]
                                    However, for any $i$, $a_i$ has to be $0$ as there's no $e_i$ term on the LHS.
                                    This would then make the entire RHS evaluate to $0$, which is not $e_1$.
                                    Thus, there can't exist any solution to this equation and $e_1 \notin W$
                                    and since $e_1 \in V$, $W \ne V$. $\square$
                              \item $0$ is in $W^\perp$ since $\braket{x, 0}=0\ \forall x \in V$.

                                    OTOH, if $x$ is nonzero, there are two cases.

                                    The first is that $x(1)=0$ and because of that $\exists n: n \ge 1 \text{ and }x(n) \ne 0$.
                                    The second is that $x(1) \ne 0$ and $\exists n: n \ge 1\text{ and }x(n)=0$.
                                    However, in both these cases notice that
                                    \begin{align*}
                                          \braket{x, e_1+e_n} & = \sum_{i=1}^{\infty} x(i)\overline{(e_1+e_n)(i)}         \\
                                                              & = x(1)\overline{(e_1+e_n)(1)}+x(n)\overline{(e_1+e_n)(1)} \\
                                                              & = x(1)+x(n)                                               \\
                                                              & \ne 0
                                    \end{align*}
                                    so $x \ne 0 \rightarrow x \notin W^\perp$.
                                    As $\{0\}^\perp=V$, we can conclude that $W \ne (W^\perp)^\perp$. $\square$
                        \end{enumerate}
            \end{enumerate}
\end{enumerate}
\end{document}
