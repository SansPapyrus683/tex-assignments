\documentclass[12pt]{article}

\input{../kz}

\begin{document}
\begin{enumerate}
      \item \begin{enumerate}
                  \item By the premise, the domains of $g_1$ and $g_2$ are equal.
                        For any $x \in X$, we know that $g_1(f(x))=g_2(f(x))=x$.
                        $f(x)=f(x)$, so $g_1=g_2$.
                        The proof that $f$ is surjective and thus can take output any
                        $y \in Y$ is in \ref{list:surjective}. $\square$
                  \item Let $X=\mathbb{R}^2$ and $Y=\mathbb{R}^3$.
                        We define $f: X \rightarrow Y$ with the formula $f((x,y))=(x,y,0)$.
                        Although we can recover the original value in $X$, by defining \\
                        $g((x,y,z))=(x,y)$, $f$ is not invertible as there isn't a function
                        $g: Y \rightarrow X$ that satisfies $f \circ g = 1_Y$.
                  \item \textbf{Bijection $\rightarrow$ Invertible} \\
                        Let $f: X \rightarrow Y$ be a bijection.
                        We will show that it is possible to construct a $g: Y \rightarrow X$ that meets the requirements for an invertible function.
                        For each $y \in Y$, we know that $\exists x \in X: f(x)=y$ since $f$ is surjective, and that this $x$ is unique since $f$ is injective.
                        Let $g(y)$ be equal to this supposed $x$.

                        $(f \circ g)(y)=y$ by definition, as we've defined $g(y)$ to be the $x \in X$ s.t. $f(x)=y$.
                        Likewise, $(g \circ f)(x)=x$ since the RHS, when plugged into $f$, yields the same result as $f(x)$.

                        \textbf{Invertible $\rightarrow$ Bijection} \\
                        We know that $f: X \rightarrow Y$ has a function $g: Y \rightarrow X$ s.t. $f(g(y))=x$ and $g(f(x))=y$.
                        It suffices to prove that $f$ is both injective and surjective.

                        For injectivity, suppose that $x, x' \in Y$ and $x \ne x'$ but $f(x)=f(x')$.
                        Let $f(x)=y$.
                        We would then have $g(y)=x$ and $g(y)=x'$.
                        However, we know that a function can only have one output per input.
                        Contradiction.
                        Thus, $f$ must be injective.

                        To prove surjectivity, every $y \in Y$ must be the output of some invocation of $f$.
                        If there was a $y \in Y$ that wasn't, then $f(g(y)) \ne y$ since $f(x) \ne y\ \forall x \in X$.
                        This is obviously untrue, so $f$ must be surjective as well. $\square$ \label{list:surjective}
            \end{enumerate}
      \item \begin{enumerate}
                  \item We first check that the dimensions are equal.
                        On the LHS, $A+B$ is an $m \times n$ matrix, which when mutliplied by $\lambda C$,
                        an $n \times k$ matrix, gives us an $m \times k$ matrix.
                        On the RHS, $AC$ and $BC$ are both $m \times k$ matrices,
                        and adding or scaling doesn't change the dimensions of a matrix,
                        so its dimensions are equal to the LHS.

                        Using the formula for matrix multiplication, we get the following expressions for the LHS and RHS respectively:
                        \begin{gather*}
                              LHS_{ij}=\sum_{x=1}^n (a_{ix}+b_{ix}) \cdot (\lambda c_{xj}) \\
                              RHS_{ij}=\sum_{x=1}^n \lambda (a_{ix}c_{xj}+b_{ix}c_{xj})
                        \end{gather*}
                        By inspection, it is clear that these two are equal. $\square$
                  \item Let's check that the dimensions are equal first.
                        $CD$ is an $n \times l$ matrix, while $BC$ is an $m \times k$ matrix.
                        Thus, $B(CD)$ and $(BC)D$ are both $m \times l$ matrices.
                        No problem there.

                        Then, we check the actual values.
                        \begin{gather*}
                              LHS_{ij}=\sum_{x=1}^n \left(b_{ix}\sum_{y=1}^k (c_{xy}d_{yj})\right) \\
                              RHS_{ij}=\sum_{y=1}^k \left(\sum_{x=1}^n (b_{ix}c_{xy}) \cdot d_{yj}\right)
                        \end{gather*}
                        To construct $RHS$ from $LHS$, we first expand the summation:
                        \[LHS_{ij}=\sum_{x=1}^n b_{ix}(c_{x1}d_{1j}+ \cdots +c_{xk}d_{kj})\]
                        Notice that the values of $d$ are constant, and that each $d$ is associated
                        with a certain set of values of $b$ and $c$.

                        Thus, we can rewrite this equation as
                        \begin{gather*}
                              LHS_{ij}=d_{1j}\sum_{x=1}^{n}(b_{ix}c_{x1}) + \cdots + d_{kj}\sum_{x=1}^{n}(b_{ix}c_{xk}) \\
                              LHS_{ij}=\sum_{y=1}^k \left(d_{yj} \sum_{x=1}^n (b_{ix}c_{xy})\right)
                        \end{gather*}
                        which is the exact expression for $RHS_{ij}$ as well. $\square$
            \end{enumerate}
      \item \begin{enumerate}
                  \item[(f)] $[T]^\gamma_\beta$ is a matrix with $1$s along the diagonal that
                        \textit{isn't the one of the identity matrix} and $0$s everywhere else.
                  \item[(g)] \[[T]^\gamma_\beta=\begin{bmatrix}
                                    1 & 0 & \cdots & 0 & 1
                              \end{bmatrix}\]
            \end{enumerate}
      \item The dimensions of $Ae_y$ are $m \times 1$, which matches up with the dimensions for a column.
            Let the result of the multiplication be $c$. Then
            \[c_{ij}=\sum_{x=1}^n A_{ix}(e_y)_{xj}\]
            Notice that $j=1$ always and thus $(e_y)_{xj}$ is $1$ if $x=y     $ and $0$ otherwise.

            We can simplify this equation to $c_{ij}=A_{iy}$,
            which by inspection is the same as taking the $y$th column of $A$. $\square$
      \item We plug in each element in $\beta$ to $T$ and put the results in the matrix.
            \[[T]^\gamma_\beta = \begin{bmatrix}
                        1 & 1 & 0 & 0 \\
                        0 & 0 & 0 & 2 \\
                        0 & 1 & 0 & 0
                  \end{bmatrix}\]
      \item \begin{enumerate}
                  \item \[\begin{bmatrix}
                                    2 & 1 & 0 \\
                                    1 & 8 & 0
                              \end{bmatrix} \cdot \begin{bmatrix}
                                    1 & 0 & 0 \\
                                    0 & 1 & 0 \\
                                    0 & 0 & 1
                              \end{bmatrix}=\begin{bmatrix}
                                    2 & 1 & 0 \\
                                    1 & 8 & 0
                              \end{bmatrix}\]
                  \item \begin{gather*}
                              A \cdot \begin{bmatrix}1 \\ 2 \\ 0\end{bmatrix} = \begin{bmatrix}4 \\ 17\end{bmatrix} = \frac{17}{2}\begin{bmatrix}1 \\ 2\end{bmatrix}-\frac{3}{2}\begin{bmatrix}3 \\ 0\end{bmatrix} \\
                              A \cdot \begin{bmatrix}1 \\ 1 \\ 1\end{bmatrix} = \begin{bmatrix}3 \\ 18\end{bmatrix} = 9\begin{bmatrix}1 \\ 2\end{bmatrix}-2\begin{bmatrix}3 \\ 0\end{bmatrix} \\
                              A \cdot \begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix} = \begin{bmatrix}0 \\ 9\end{bmatrix} = \frac{9}{2}\begin{bmatrix}1 \\ 2\end{bmatrix}-\frac{3}{2}\begin{bmatrix}3 \\ 0\end{bmatrix}
                        \end{gather*}
                        Putting all these linear combinations together gives us
                        \[[L_A]^{\gamma_2}_{\gamma_3}=\begin{bmatrix}
                                    \frac{17}{2} & 9  & \frac{9}{2}  \\
                                    -\frac{3}{2} & -2 & -\frac{3}{2}
                              \end{bmatrix}\]
            \end{enumerate}
      \item \begin{enumerate}
                  \item Suppose we had a nonzero element $v \in V$ s.t. $T(v)=\vec{0}$ and $\exists v' \in V: T(v')=v$.
                        If the either the rank or nullity of $T$ was $0$, then this assumption would be immediately
                        violated and we'd have $\text{Null}(T) \cap \text{Im}(T)=\{\vec{0}\}$.

                        However, if it wasn't the case, then $T(T(v'))=\vec{0}$ and thus $v' \in \text{Ker}(T \circ T)$ but not $\text{Ker}(T)$.
                        By the dimension theorem, \[\dim \text{Im}(T)+\dim \text{Null}(T)=\dim V=\dim \text{Im}(T \circ T)+\dim \text{Null}(T \circ T)\]
                        This implies that $\dim \text{Null}(T)=\dim \text{Null}(T \circ T)$.

                        Notice that if $T(v)=\vec{0}$, then $T(T(v))=\vec{0}$ as well.
                        Since we also proved that $\exists v \in V: T(v) \ne \vec{0} \land T(T(v))=\vec{0}$,
                        this means that $\text{Null}(T) \subset \text{Null}(T \circ T)$ and by
                        extension $\dim \text{Null}(T) <\dim \text{Null}(T \circ T)$.
                        Contradiction.

                        Thus, $\text{Null}(T) \cap \text{Im}(T)=\{\vec{0}\}$. $\square$

                        The deduction follows naturally from what we've just proven and a lemma of the Dimension Theorem.
                  \item It suffices to prove that for some positive integer $k$, $\dim \text{Im}(T^k)=\dim \text{Im}(T^{2k})$.
                        Then, by creating a new linear transformation $T'=T^k$ and the result in the previous part, we know that
                        $\text{Null}(T^k) \cap \text{Im}(T^k) = \{\vec{0}\}$ and $V=\text{Null}(T^k) \oplus \text{Im}(T^k)$.

                        We know that the rank of $T^{k+1}$ can never be greater than the rank of $T^k$.
                        Doing so would violate the dimension theorem.
                        Thus, the function $f: \mathbb{Z}_{>0} \rightarrow \mathbb{Z}_{\ge 0}$
                        defined by $f(k)=\dim \text{Im}(T^{k})$ must be nonincreasing.

                        We propose that there exists a $k$ s.t. $f(x)=f(2x)\ \forall x \ge k$.
                        If this is true, then we know that for this value of $k$,
                        $\dim \text{Im}(T^k)=\dim \text{Im}(T^{2k})$.

                        Suppose that $f(k) \ne f(2k)\ \forall k \in \mathbb{Z}_{>0}$.

                        Then, if $f(1)=a$, then $f(1 \cdot 2)=a-1$ by this condition and the previous nonincreasing property.
                        At some point, we're going to hit $f(k)=0$, where we can't go down any further and have to set $f(2k)=0$, which results in a contradiction.
                        Thus, there must be some $k$ s.t. $f(k)=f(2k)$ and by extension $\dim \text{Im}(T^k)=\dim \text{Im}(T^{2k})$. $\square$
            \end{enumerate}
      \item \begin{enumerate}
                  \item In class, it was established that $[T_1+T_2]^\gamma_\beta=[T_1]^\gamma_\beta+[T_2]^\gamma_\beta$ and that
                        $[\lambda T]^\gamma_\beta = \lambda [T]^\gamma_\beta$.
                        Since $\mathbb{T}(T)=[T]^\gamma_\beta$, the above two facts demonstrate that $\mathbb{T}$ is a linear transformation.

                  \item It suffices to show that $\mathbb{T}$ is bijective.

                        \textbf{Injective} \\
                        If two linear transformations $T_1$ and $T_2$ are different, then there must be some $v_i \in \beta: T_1(v_i) \ne T_2(v_i)$.
                        For this $v_i$, since the two $T$s produce different results, it must also mean that the linear combos for them in $\gamma$ are different.

                        As $\mathbb{T}$ is defined by the linear combinations in $\gamma$ that result in these transformed bases,
                        this also means that $\mathbb{T}(T_1) \ne \mathbb{T}(T_2)$.

                        \textbf{Surjective} \\
                        For any $n \times m$ matrix $M$, we can define \[T(v_i)=\sum_{x=1}^{n} m_{xi}w_x\]
                        Since a linear transformtion is uniquely defined by how it acts on all elements of a basis,
                        we've just defined a valid linear transformation $T$ s.t. $\mathbb{T}(T)=M$. $\square$
            \end{enumerate}
\end{enumerate}
\end{document}
