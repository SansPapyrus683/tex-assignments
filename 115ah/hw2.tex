\documentclass[12pt]{article}

\input{../kz}

\begin{document}
\begin{enumerate}
      \item Say we have two vectors $y$ and $y'$ s.t. $x+y=0$ and $x+y'=0$.
            Then, we know that $x+y=x+y'$ and then $y=y'$ by the cancellation law for vector addition.
      \item No, as the addition operation is not commutative.
            As a counter example, let $x=(1,1)$ and $y=(2,3)$.
            Then, we have two different results when computing $x+y$ and $y+x$:
            \begin{gather*}
                  x+y=(1+2,2 \cdot 1 + 3 \cdot 3)=(3,11) \\
                  y+x=(2+1, 2 \cdot 2 + 3 \cdot 1)=(3,7)
            \end{gather*}
      \item \begin{itemize}
                  \item[1-4.] These are satisfied because we've previously proved that $\mathbb{C}$ is a field.
                  \item[5.] \textbf{Scalar Identity}
                        \begin{align*}
                              1 \cdot (a+bi) & =(1 \cdot a)+(1 \cdot b)i \\
                                             & =a+bi
                        \end{align*}
                  \item[6.] \textbf{Associativity w.r.t. Scalar Multiplication}
                        \begin{align*}
                              (xy)
                              \cdot (a+bi) & = (xy \cdot a)+(xy \cdot b)i \\
                                           & = xya+xybi                   \\
                                           & = x \cdot (ya+ybi)           \\
                                           & = x \cdot (y \cdot (a+bi))
                        \end{align*}
                  \item[7.] \textbf{Distributivity P1}
                        \begin{align*}
                              x((a+bi)+(c+di)) & = x((a+c)+(b+d)i)   \\
                                               & = (xa+xc)+(xb+xd)i  \\
                                               & = (xa+xbi)+(xc+xdi) \\
                                               & = x(a+bi)+x(c+di)
                        \end{align*}
                  \item[8.] \textbf{Distributivity P2}
                        \begin{align*}
                              (x+y)(a+bi)
                               & =((x+y)a)+((x+y)b)i \\
                               & = (xa+ya)+(xb+yb)i  \\
                               & = (xa+xbi)+(ya+ybi) \\
                               & = x(a+bi)+y(a+bi)
                        \end{align*}
            \end{itemize}
      \item \begin{enumerate}
                  \item If we have two functions $f_1$ and $f_2$, we can define the result of addition
                        on them to be a third function $g: S \rightarrow V$ that has the formula
                        \[g(s)=f_1(s)+f_2(s)\ \forall s \in S\]
                  \item We also define scalar multiplication of elements in a similar manner,
                        with $\lambda \cdot f$ having the result of a function $g: S\rightarrow V$ with the formula
                        \[g(s)=\lambda f(s)\ \forall s \in S\]
                  \item For all these axioms, domain and codomain are the same since we aren't doing anything funny with those.
                        \begin{enumerate}[label=\arabic*]
                              \item \textbf{Commutativity} \\
                                    Consider $g(s)=f_1(s)+f_2(s)$ and $g(s)=f_2(s)+f_1(s)$.
                                    For all values of $s$, we know that these two expressions are equal due to values of $V$ being in a vector space.
                              \item \textbf{Associativity} \\
                                    Similarly, since elements of a vector space are associative under addition, $g:S \rightarrow V$ is the same no matter what order you perform addition.
                              \item \textbf{Additive Identity} \\
                                    The additive identity is $f_0(s)=\vec{0}\ \forall s \in S$.
                                    If we add this to any $f$, we have
                                    \[g(s)=f(s)+f_0(s)=f(s)+\vec{0}=f(s)\]
                              \item \textbf{Additive Inverse} \\
                                    The additive inverse for any function is $f^-(s)=-f(s)$, where $-f(s)$ denotes the additive inverse for $f(s) \in V$.
                                    Adding this to any $f$ gives us
                                    \[g(s)=f(s)+f^-(s)=f(s)+(-f(s))=\vec{0}\]
                              \item \textbf{Scalar Identity} \\
                                    The scalar identity is just $1$.
                                    Multiplying an $f$ by $1$ gives us
                                    \[g(s)=f(s) \cdot 1=f(s)\]
                                    since $V$ is a vector space.
                              \item \textbf{Associativity w.r.t. Scalar Multiplication} \\
                                    $(ab) \cdot f(s)$ evaluates to $g(s)=(ab) \cdot f(s)$,
                                    while $a(b \cdot f(s))$ evaluates to $g(s)=a(b \cdot f(s))$.
                                    We know these two to be equal since $V$ is a vector space and has the property we're trying to prove.
                              \item \textbf{Distributivity P1 \& P2} \\
                                    Taking $g(s)=a(f(s)+f'(s))$, we know that this is equal to $g(s)=af(s)+af'(s)$ since $f(s), f'(s) \in V$.
                                    The proof for the second distributivity property is basically the same thing.
                        \end{enumerate}
            \end{enumerate}
      \item \begin{enumerate}
                  \item Since $(x,y), (z,w) \in R_U$, we know that $x-y \in U$ and $z-w \in U$.
                        Since $U$ is a subspace, we know that $(x-y)+(z-w)=(x+z)-(y+w) \in U$ as well.
                        Thus $x+z \sim_R y+w$ and by the result in WS1 we know that $[x+z]=[y+w]$. $\square$

                  \item If $(x,y) \in R_U$, then $x-y \in U$ and $\lambda(x-y)=\lambda x -\lambda y \in U$ due to it being a subspace.
                        Thus, we have $\lambda x \sim_R \lambda y \rightarrow [\lambda x]=[\lambda y]$. $\square$

                  \item When working with equivalence classes we have to establish that the operations' results
                        don't depend on the choice of representative for these classes.

                        We've proved that addition and scaling of equivalence classes doesn't depend on the choice
                        of representative for these classes, so the operations are well defined.
            \end{enumerate}
      \item \begin{enumerate}
                  \item $S+T=\{(0,0)\}$, while $S \cup T=\{(1,0),(-1,0)\}$
                  \item Let $S$ and $T$ be defined as follows:
                        \begin{gather*}
                              S=\{(x, y) \in R \times R\ |\ x=y\} \\
                              T=\{(x, y) \in R \times R\ |\ x=2y\}
                        \end{gather*}
                        Even though these are both subspaces, their union isn't.
                        For example, adding $(1, 1)$ from $S$ and $(2, 1)$ fromm $T$ gives $(3, 2)$, which is not in $S \cup T$.
                  \item First off, we know that $\vec{0}$ is in $S+T$ since we can add $\vec{0}$ from $S$ and $\vec{0}$ from $T$.

                        Now we prove closure under addition and scalar multiplication.
                        Say we have $v_1, v_2 \in S+T$.
                        This means that that we can represent them as the following:
                        \begin{gather*}
                              v_1=a_1 s_1 + b_1 t_1\ \exists a_1, b_1 \in \mathbb{R}, s_1 \in S, t_1 \in T \\
                              v_2=a_2 s_2 + b_2 t_2\ \exists a_2, b_2 \in \mathbb{R}, s_2 \in S, t_2 \in T
                        \end{gather*}
                        Since $S$ and $T$ are vector spaces, we know that $a_1 s_1 +a_2 s_2 \in S$ and $b_1 t_1 + b_2 t_2 \in T$,
                        so $v_1+v_2$ can be represented as the sum of two elements from $S$ and $T$. $\square$
                  \item We know that $U_1, U_2 \subset W$, so any $u_1 \in U_1$ and $u_2 \in U_2$ we know that $u_1+u_2 \in W$
                        since $u_1, u_2 \in W$ as well.
                        Since $U_1+U_2$ is literally defined as all possible combinations made by summing the elements from $U_1$ and $U_2$,
                        we know that $U_1+U_2 \subset W$. $\square$
            \end{enumerate}
      \item \begin{enumerate}
                  \item True.
                  \item False.
                  \item False.
                  \item False.
                  \item True.
                  \item False
                  \item True.
            \end{enumerate}
      \item Clearly, $\vec{0}$ exists in $U$, since a matrix of all $0$'s is symmetric.

            We also have to show that $\lambda_1 X + \lambda_2 X' = Y \in U$, where $X$ and $X'$ are in $U$.
            In other words, we must prove that the resulting matrix is also symmetric.
            \[Y_{ij}=\lambda_1 X_{ij}+\lambda_2 X'_{ij}=\lambda_1 X_{ji}+\lambda_2 X'_{ji}=Y_{ji}\]
            Thus, $Y$ is also symmetric and in $U$.
      \item \hfill$\begin{aligned}[t]
                        T(v_1)+T(v_2) & =[v_1]+[v_2] & cT(v) & =c \cdot [v] \\
                                      & = [v_1+v_2]  &       & =[c \cdot v] \\
                                      & = T(v_1+v_2) &       & =T(cV)\quad
                  \end{aligned}$\hfill\null \\
            Both of the properties are true, and thus $T$ is a linear transformation. $\square$

            Since the zero vector of $V/U$ is $[\vec{0}]$, the kernel of $T$ consists of every
            vector in that equivalence class.
      \item Since $T(0v)=0T(v)\ \forall v \in V$, we know that $\vec{0}$ must be in $\text{Im}(t)$.

            We also have to prove that $\text{Im}(T)$ is closed under addition and scalar multiplication.
            \begin{gather*}
                  T(v_1)+T(v_2)=T(v_1+v_2) \in \text{Im}(t) \\
                  \lambda T(v)=T(\lambda v) \in \text{Im}(t)\quad\square
            \end{gather*}
\end{enumerate}
\end{document}
